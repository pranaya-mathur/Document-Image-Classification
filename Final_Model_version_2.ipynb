{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "983JHu-2OttU"
   },
   "outputs": [],
   "source": [
    "label = {0:\"letter\",1:\"form\",2:\"email\",3:\"handwritten\",4:\"advertisement\",\n",
    "         5:\"scientific report\",6:\"scientific publication\",7:\"specification\",\n",
    "         8:\"file folder\",9:\"news article\",10:\"budget\",11:\"invoice\",12:\"presentation\",\n",
    "         13:\"questionnaire\",14:\"resume\",15:\"memo\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "hxhohUIJlQ_W",
    "outputId": "d267a9f4-e7ae-4910-b08d-aaa3d11dbd07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-20 06:58:30--  https://aip.scitation.org/pb-assets/images/sample_supplementary_material-1549471999507.jpg\n",
      "Resolving aip.scitation.org (aip.scitation.org)... 104.17.163.62, 104.17.164.62\n",
      "Connecting to aip.scitation.org (aip.scitation.org)|104.17.163.62|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 184842 (181K) [image/jpeg]\n",
      "Saving to: ‘sample_supplementary_material-1549471999507.jpg’\n",
      "\n",
      "\r",
      "          sample_su   0%[                    ]       0  --.-KB/s               \r",
      "sample_supplementar 100%[===================>] 180.51K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2019-11-20 06:58:30 (7.15 MB/s) - ‘sample_supplementary_material-1549471999507.jpg’ saved [184842/184842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://aip.scitation.org/pb-assets/images/sample_supplementary_material-1549471999507.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "VLbsc0KklXmv",
    "outputId": "28a12b79-291e-476c-8a2b-3c29e4f2fe9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, Input, BatchNormalization, Dropout, Dense, GlobalAvgPool2D, GlobalMaxPooling2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop, Adagrad, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "9oR5QGLAmkgp",
    "outputId": "4f740bdc-2980-4505-ac8a-2cf5a366fe5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24v_PXuLmxco"
   },
   "outputs": [],
   "source": [
    "def result(image):\n",
    "\n",
    "  # We will load our model with their respective weights\n",
    "\n",
    "  # Holistic Model Loading\n",
    "  json_file = open('drive/My Drive/Document Image Classification Files/model_holistic.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  model_holistic = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  model_holistic.load_weights(\"drive/My Drive/Document Image Classification Files/ content model_holistic.h5\")\n",
    "  print(\"Loaded model from disk Holistic\")\n",
    "  # Header Region Specific Model Loading\n",
    "\n",
    "  json_file = open('drive/My Drive/Document Image Classification Files/model_header.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  model_header = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  model_header.load_weights(\"drive/My Drive/Document Image Classification Files/ content model_header.h5\")\n",
    "  print(\"Loaded model from disk Header\")\n",
    "\n",
    "  # Bottom Region Specific Trained Model Loading\n",
    "\n",
    "  json_file = open('drive/My Drive/Document Image Classification Files/model_bottome.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  model_bottom = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  model_bottom.load_weights(\"drive/My Drive/Document Image Classification Files/ content model_bottome.h5\")\n",
    "  print(\"Loaded model from disk Bottom\")\n",
    "\n",
    "  # Left Region Specific Trained Model Loading\n",
    "\n",
    "  json_file = open('drive/My Drive/Document Image Classification Files/model_left.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  model_left = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  model_left.load_weights(\"drive/My Drive/Document Image Classification Files/ content model_left.h5\")\n",
    "  print(\"Loaded model from disk Left\")\n",
    "\n",
    "  # Right Region Specific Trained Model Loading\n",
    "\n",
    "  json_file = open('drive/My Drive/Document Image Classification Files/model_right.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  model_right = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  model_right.load_weights(\"drive/My Drive/Document Image Classification Files/ content model_right.h5\")\n",
    "  print(\"Loaded model from disk Right\")\n",
    "\n",
    "  # Meta Classifier Trained Model Loading\n",
    "\n",
    "  json_file = open('drive/My Drive/Document Image Classification Files/model_.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  model_meta = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  model_meta.load_weights(\"drive/My Drive/Document Image Classification Files/model_metaclassifier.h5\")\n",
    "  print(\"Loaded model from disk Meta\")\n",
    "\n",
    "  img = plt.imread(image)\n",
    "  # plt.imshow(img)\n",
    "\n",
    "  # for whole image\n",
    "  img_whole = cv2.resize(img,(256,256))\n",
    "  img_whole = np.reshape(img_whole,[1,256,256,3])/255.\n",
    "  \n",
    "  # for header part of an image\n",
    "  img_header = img[:256,:,:]\n",
    "  img_header_ = cv2.resize(img_header,(256,256))\n",
    "  img_header_= np.reshape(img_header_,[1,256,256,3])/255.\n",
    "\n",
    "  # for bottom part of an image\n",
    "  img_bottom = img[-256:,:,:]\n",
    "  img_bottom_ = cv2.resize(img_bottom,(256,256))\n",
    "  img_bottom_= np.reshape(img_bottom_,[1,256,256,3])/255.\n",
    "\n",
    "  # for right part of an image\n",
    "  img_right = img[:,-256:,:]\n",
    "  img_right_ = cv2.resize(img_right,(256,256))\n",
    "  img_right_= np.reshape(img_right_,[1,256,256,3])/255.\n",
    "\n",
    "  # for left part of an image\n",
    "  img_left = img[:,:256,:]\n",
    "  img_left_ = cv2.resize(img_left,(256,256))\n",
    "  img_left_= np.reshape(img_left_,[1,256,256,3])/255.\n",
    "\n",
    "  # Predicting results on different parts of the image\n",
    "  features_whole = model_holistic.predict(img_whole)\n",
    "  features_bottom = model_bottom.predict(img_bottom_)\n",
    "  features_header = model_bottom.predict(img_header_)\n",
    "  features_right = model_bottom.predict(img_right_)\n",
    "  features_left = model_bottom.predict(img_left_)\n",
    "\n",
    "  # Stacking these predictions\n",
    "  stacking = np.hstack((features_whole,features_header,features_bottom,features_left,features_right))\n",
    "\n",
    "  # Meta Classifier\n",
    "  predictions = model_meta.predict(stacking)\n",
    "  label = {0:\"letter\",1:\"form\",2:\"email\",3:\"handwritten\",4:\"advertisement\",\n",
    "         5:\"scientific report\",6:\"scientific publication\",7:\"specification\",\n",
    "         8:\"file folder\",9:\"news article\",10:\"budget\",11:\"invoice\",12:\"presentation\",\n",
    "         13:\"questionnaire\",14:\"resume\",15:\"memo\"}\n",
    "\n",
    "  result = np.argmax(predictions,axis=1)\n",
    "  print(result)\n",
    "  if result[0] in label:\n",
    "    print(label[result[0]])\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "Qb04yUjSnP0g",
    "outputId": "87ac0aaf-0464-4d7e-83f7-aac126be7c35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk Holistic\n",
      "Loaded model from disk Header\n",
      "Loaded model from disk Bottom\n",
      "Loaded model from disk Left\n",
      "Loaded model from disk Right\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Loaded model from disk Meta\n",
      "[15]\n",
      "memo\n"
     ]
    }
   ],
   "source": [
    "result(\"/content/sample_supplementary_material-1549471999507.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdzB6vEpnTtC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Model version 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
